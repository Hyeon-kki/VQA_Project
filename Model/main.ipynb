{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhyeon_kki\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/workspace/QA_project/Model/wandb/run-20240226_091433-maqeb4uo</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29/runs/maqeb4uo' target=\"_blank\">eager-deluge-2</a></strong> to <a href='https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29' target=\"_blank\">https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29/runs/maqeb4uo' target=\"_blank\">https://wandb.ai/hyeon_kki/VQA%20baseline%20%28train%29/runs/maqeb4uo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.run.save without any arguments is deprecated.Changes to attributes are automatically persisted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "\n",
    "# wandb.init(project='VQA baseline (train)')\n",
    "wandb.run.name = 'First wandb'\n",
    "wandb.run.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59076/2246612629.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/root/anaconda3/envs/QA/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current device is cuda\n",
      "current device is cuda\n",
      "current device is cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5618/5618 [32:48<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.2197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5618/5618 [32:52<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Loss: 0.1687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5618/5618 [32:53<00:00,  2.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Loss: 0.1552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision.models as models # 이미지\n",
    "from torchvision import transforms # 아마지의 크기, 증강, 정규화를 진행하고 토치텐서로 출력하기 위해 사용\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import GPT2Tokenizer, GPT2Model # 텍스트\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from Modules.train import train\n",
    "from Modules.inference import inference\n",
    "from Modules.VQADataset import VQADataset\n",
    "from Modules.VQAModel import VQAModel\n",
    "from Modules.load_data import load_data\n",
    "\n",
    "# init config setting\n",
    "CFG = {\n",
    "  \"learning_rate\": 5e-5,\n",
    "  \"epochs\": 3,\n",
    "  \"batch_size\": 64\n",
    "}\n",
    "wandb.config.update(CFG)\n",
    "\n",
    "# device\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"current device is {device}\")\n",
    "\n",
    "train_df, test_df, sample_submission, train_img_path, test_img_path = load_data()\n",
    "\n",
    "# dataset & dataloaders\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "# len(tokenizer)는 토크나이저가 인식하는 전체 토큰의 개수를 나타낸다.\n",
    "# 즉, 토크나이저 어휘집의 크기와 동일합니다. 이 숫자는 모델이 학습된 토크나이저에 따라 달라진다.\n",
    "# GPT2 토크나이저 어휘집 50258\n",
    "vocab_size = len(tokenizer)\n",
    "\n",
    "transform = transforms.Compose([ # 이미지 변환 함수들을 조합하여 하나의 변환 함수로 만들어주는 함수이다.\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),])\n",
    "\n",
    "train_dataset = VQADataset(train_df, tokenizer, transform, train_img_path, is_test=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Model\n",
    "model = VQAModel(vocab_size).to(device)\n",
    "\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):\n",
    "    avg_loss = train(model, train_loader, optimizer, criterion)\n",
    "    wandb.log({\"train_loss\": avg_loss}, step=epoch)\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {avg_loss:.4f}\") # fprint 처리 잘하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 633/633 [02:57<00:00,  3.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Dataset & DataLoader\n",
    "test_dataset = VQADataset(test_df, tokenizer, transform, test_img_path, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# inference\n",
    "preds = inference(model, test_loader)\n",
    "\n",
    "no_pad_output = []\n",
    "for pred in preds:\n",
    "    output = pred[pred != 50257] # [PAD] token 제외\n",
    "    no_pad_output.append(tokenizer.decode(output).strip()) # 토큰 id -> 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['answer'] = no_pad_output\n",
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = pd.read_csv('solution.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
